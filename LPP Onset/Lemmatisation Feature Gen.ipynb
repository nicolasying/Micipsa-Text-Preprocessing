{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stimuli Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "il ['chapitre_n__originChapitre', '26_n__origin26', '__PRONOUN____originIl', '__PRONOUN____originy', 'avoir_v__originavait', '__punctuation____origin,', 'à_prep__originà', 'côté_n__origincôté', 'du_det__origindu', 'puits_n__originpuits']\n",
      "il ['26_n__origin26', '__PRONOUN____originIl', '__PRONOUN____originy', 'avoir_v__originavait', '__punctuation____origin,', 'à_prep__originà', 'côté_n__origincôté', 'du_det__origindu', 'puits_n__originpuits', '__punctuation____origin,']\n",
      "et ['chapitre_n__originChapitre', '27_n__origin27', '__CCONJ____originEt', 'maintenant_adv__originmaintenant', '__punctuation____origin,', 'bien_adv__originbien', 'sûr_adj__originsûr', '__punctuation____origin,', 'cela_pro__originça', 'faire_v__originfait']\n",
      "et ['27_n__origin27', '__CCONJ____originEt', 'maintenant_adv__originmaintenant', '__punctuation____origin,', 'bien_adv__originbien', 'sûr_adj__originsûr', '__punctuation____origin,', 'cela_pro__originça', 'faire_v__originfait', 'six_n__originsix']\n"
     ]
    }
   ],
   "source": [
    "# sess_1 = pd.read_csv('1.csv')\n",
    "# sess_2 = pd.read_csv('2.csv')\n",
    "#sess_3 = pd.read_csv('3.csv')\n",
    "#sess_4 = pd.read_csv('4.csv')\n",
    "#sess_5 = pd.read_csv('5.csv')\n",
    "#sess_6 = pd.read_csv('6.csv')\n",
    "#sess_7 = pd.read_csv('7.csv')\n",
    "#sess_8 = pd.read_csv('8.csv')\n",
    "sess_9 = pd.read_csv('9.csv')\n",
    "\n",
    "file = open('../TextFineTuning/lpp_lem.txt', mode='r')\n",
    "lemmed = file.read()\n",
    "file.close()\n",
    "\n",
    "lemmed = lemmed.split()\n",
    "# -1, 1925, 3986, 6227, 8227, 10079, 12354, 14487 \n",
    "word_idx = 16347\n",
    "ite = iter(sess_9.word)\n",
    "target_word = next(ite)\n",
    "lemma_toadd = []\n",
    "categ_toadd = []\n",
    "\n",
    "while word_idx < len(lemmed) - 1:\n",
    "    word_idx += 1\n",
    "    if len(lemmed[word_idx]) < 9:\n",
    "        continue\n",
    "    try:\n",
    "        categ, word = lemmed[word_idx].split('__origin')\n",
    "        lemma, categ = categ.rsplit('_', 1)\n",
    "        \n",
    "        if target_word[0] == '-':\n",
    "            target_word = target_word[1:]\n",
    "        if word[0] == '-':\n",
    "            word = word[1:]\n",
    "        if word.lower() == target_word.lower():\n",
    "            lemma_toadd.append(lemma)\n",
    "            categ_toadd.append(categ)\n",
    "            try:\n",
    "                target_word = next(ite)\n",
    "            except StopIteration:\n",
    "                break\n",
    "        elif lemmed[word_idx][:10] == '__punctuat':\n",
    "            continue\n",
    "        else:\n",
    "            print(target_word, lemmed[word_idx:word_idx+10])\n",
    "    except:\n",
    "        print(lemmed[word_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18512"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1760"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lemma_toadd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_9['lemma'] = lemma_toadd\n",
    "sess_9['categ_alt'] = categ_toadd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_1.to_csv('1_lem_auto.csv')\n",
    "sess_2.to_csv('2_lem_auto.csv')\n",
    "sess_3.to_csv('3_lem_auto.csv')\n",
    "sess_4.to_csv('4_lem_auto.csv')\n",
    "sess_5.to_csv('5_lem_auto.csv')\n",
    "sess_6.to_csv('6_lem_auto.csv')\n",
    "sess_7.to_csv('7_lem_auto.csv')\n",
    "sess_8.to_csv('8_lem_auto.csv')\n",
    "sess_9.to_csv('9_lem_auto.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New LPP Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpp_onset = pd.concat([pd.read_csv(str(i) + '_lem_auto.csv') for i in range(1,10)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpp_onset.groupby(['lemma', 'categ', 'categ_alt', 'Contentword']).count()[['word']].to_csv('lpp_correction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrected Onset File with WOLF vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_dict = {\n",
    "    \"ADJ\": 'a',\n",
    "    \"ADV\": 'b',\n",
    "    \"NOM\": 'n',\n",
    "    \"VER\": 'v'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sying/root/anaconda3/envs/micipsa/lib/python3.6/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,10):\n",
    "    session = pd.read_csv(str(i)+'_WOLF.csv', index_col=0)\n",
    "    session_cw = session.loc[session.proposed_CW=='Y']\n",
    "    session_cw.loc[:,\"proposed_categ\"] = session_cw.proposed_categ.map(POS_dict)\n",
    "    session_cw.loc[:, \"word\"] = session_cw.proposed_lemma + '_' + session_cw.proposed_categ\n",
    "    session_cw[['word', 'onset', 'offset']].to_csv('./Context Lemma Onsets/' + str(i)+'_onset_wolf.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embedding Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mDecorrelation\u001b[0m/              \u001b[01;34mSimilarity-Association-Benchmarks\u001b[0m/\n",
      "\u001b[01;34mLePetitPrince_Pallier2018\u001b[0m/  \u001b[01;34mTextFineTuning\u001b[0m/\n",
      "\u001b[01;34mMango\u001b[0m/                      \u001b[01;34mword2vec\u001b[0m/\n",
      "\u001b[01;34mpycortex\u001b[0m/                   \u001b[01;34mWordNetEmbeddings\u001b[0m/\n",
      "\u001b[01;34mROI\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_model = pd.read_csv('../../Decorrelation/French_POS/sim_56k_634d.txt', index_col=0, header=None, skiprows=1, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_model = pd.read_csv('../../Decorrelation/French_POS/mix_embedding_24519.txt', index_col=0, header=None, skiprows=1, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session 1 : 9 words among 227 not found in embedding mix_verb\n",
      "Session 2 : 19 words among 274 not found in embedding mix_verb\n",
      "Session 3 : 15 words among 313 not found in embedding mix_verb\n",
      "Session 4 : 17 words among 306 not found in embedding mix_verb\n",
      "Session 5 : 10 words among 258 not found in embedding mix_verb\n",
      "Session 6 : 8 words among 296 not found in embedding mix_verb\n",
      "Session 7 : 9 words among 331 not found in embedding mix_verb\n",
      "Session 8 : 12 words among 278 not found in embedding mix_verb\n",
      "Session 9 : 14 words among 335 not found in embedding mix_verb\n"
     ]
    }
   ],
   "source": [
    "# Select only NOUNS / VERBS / ADJs\n",
    "\n",
    "# Configurations\n",
    "embed_type = 'mix_verb'\n",
    "model = mix_model\n",
    "embed_dim = len(model.columns)\n",
    "embed_voc = len(model)\n",
    "\n",
    "for i in range(1,10):\n",
    "    session = pd.read_csv('./Context Lemma Onsets/' + str(i) + '_onset_wolf.csv')\n",
    "    # session = session.loc[session.word.str.endswith('a') | session.word.str.endswith('b')]\n",
    "    session = session.loc[session.word.str.endswith('v')]\n",
    "    session[\"duration\"] = session.offset - session.onset\n",
    "    features = pd.merge(session, model, left_on='word', right_on=0, how='left')\n",
    "    print(\"Session\", i, \":\", sum(features.isna().any(axis=1)), \"words among\", len(features), \"not found in embedding\", embed_type)\n",
    "    features.dropna(inplace=True)\n",
    "    for d in range(1, embed_dim+1):\n",
    "        prefix = str(i) + '_' + embed_type + '_dim' + str(embed_dim) + '_voc' + str(embed_voc) + '_d'\n",
    "        features[['onset', 'duration', d]].rename(columns={d: 'amplitude'}).to_csv('./Features/' + prefix + str(d) + '.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session 1 : 36 words among 725 not found in embedding sim\n",
      "Session 2 : 30 words among 812 not found in embedding sim\n",
      "Session 3 : 32 words among 860 not found in embedding sim\n",
      "Session 4 : 27 words among 762 not found in embedding sim\n",
      "Session 5 : 30 words among 732 not found in embedding sim\n",
      "Session 6 : 33 words among 902 not found in embedding sim\n",
      "Session 7 : 24 words among 819 not found in embedding sim\n",
      "Session 8 : 30 words among 712 not found in embedding sim\n",
      "Session 9 : 27 words among 802 not found in embedding sim\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,10):\n",
    "    session = pd.read_csv('./Context Lemma Onsets/' + str(i) + '_onset_wolf.csv')\n",
    "    session[\"duration\"] = session.offset - session.onset\n",
    "    features = pd.merge(session, sim_model, left_on='word', right_on=0, how='left')\n",
    "    print(\"Session\", i, \":\", sum(features.isna().any(axis=1)), \"words among\", len(features), \"not found in embedding\", embed_type)\n",
    "    features.dropna(inplace=True)\n",
    "    for d in range(1, embed_dim+1):\n",
    "        prefix = str(i) + '_' + embed_type + '_dim' + str(embed_dim) + '_voc' + str(embed_voc) + '_d'\n",
    "        features[['onset', 'duration', d]].rename(columns={d: 'amplitude'}).to_csv('./Features/' + prefix + str(d) + '.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "asn_model = pd.read_csv('../../Decorrelation/French_POS/asn_embedding_24519.txt', index_col=0, header=None, skiprows=1, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "embed_type = 'asn'\n",
    "embed_dim = len(asn_model.columns)\n",
    "embed_voc = len(asn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session 1 : 48 words among 725 not found in embedding asn\n",
      "Session 2 : 47 words among 812 not found in embedding asn\n",
      "Session 3 : 38 words among 860 not found in embedding asn\n",
      "Session 4 : 37 words among 762 not found in embedding asn\n",
      "Session 5 : 48 words among 732 not found in embedding asn\n",
      "Session 6 : 60 words among 902 not found in embedding asn\n",
      "Session 7 : 35 words among 819 not found in embedding asn\n",
      "Session 8 : 37 words among 712 not found in embedding asn\n",
      "Session 9 : 41 words among 802 not found in embedding asn\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,10):\n",
    "    session = pd.read_csv('./Context Lemma Onsets/' + str(i) + '_onset_wolf.csv')\n",
    "    session[\"duration\"] = session.offset - session.onset\n",
    "    features = pd.merge(session, asn_model, left_on='word', right_on=0, how='left')\n",
    "    print(\"Session\", i, \":\", sum(features.isna().any(axis=1)), \"words among\", len(features), \"not found in embedding\", embed_type)\n",
    "    features.dropna(inplace=True)\n",
    "    for d in range(1, embed_dim+1):\n",
    "        prefix = str(i) + '_' + embed_type + '_dim' + str(embed_dim) + '_voc' + str(embed_voc) + '_d'\n",
    "        features[['onset', 'duration', d]].rename(columns={d: 'amplitude'}).to_csv('./Features/' + prefix + str(d) + '.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['1_n', 'Monseigneur_n', 'Aaron_n', 'abbé_n', 'abc_n', 'Abel_n',\n",
       "       'Abel_n', 'Abel_n', 'Abkhazie_n', 'abondance_n',\n",
       "       ...\n",
       "       'Étienne_n', 'Étienne_n', 'Étienne_n', 'Ève_n', 'Édimbourg_n',\n",
       "       'Égypte_n', 'Élisabeth_n', 'Épiaire_n', 'Éthiopie_n', 'Étienne_n'],\n",
       "      dtype='object', name=0, length=3220)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asn_model.index[asn_model.index.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_model = pd.read_csv('../../Decorrelation/French_POS/sig_embedding_24519.txt', index_col=0, header=None, skiprows=1, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "embed_type = 'sig'\n",
    "embed_dim = len(sig_model.columns)\n",
    "embed_voc = len(sig_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session 1 : 48 words among 725 not found in embedding sig\n",
      "Session 2 : 47 words among 812 not found in embedding sig\n",
      "Session 3 : 38 words among 860 not found in embedding sig\n",
      "Session 4 : 37 words among 762 not found in embedding sig\n",
      "Session 5 : 48 words among 732 not found in embedding sig\n",
      "Session 6 : 60 words among 902 not found in embedding sig\n",
      "Session 7 : 35 words among 819 not found in embedding sig\n",
      "Session 8 : 37 words among 712 not found in embedding sig\n",
      "Session 9 : 41 words among 802 not found in embedding sig\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,10):\n",
    "    session = pd.read_csv('./Context Lemma Onsets/' + str(i) + '_onset_wolf.csv')\n",
    "    session[\"duration\"] = session.offset - session.onset\n",
    "    features = pd.merge(session, sig_model, left_on='word', right_on=0, how='left')\n",
    "    print(\"Session\", i, \":\", sum(features.isna().any(axis=1)), \"words among\", len(features), \"not found in embedding\", embed_type)\n",
    "    features.dropna(inplace=True)\n",
    "    for d in range(1, embed_dim+1):\n",
    "        prefix = str(i) + '_' + embed_type + '_dim' + str(embed_dim) + '_voc' + str(embed_voc) + '_d'\n",
    "        features[['onset', 'duration', d]].rename(columns={d: 'amplitude'}).to_csv('./Features/' + prefix + str(d) + '.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mix_model = pd.read_csv('../../Decorrelation/French_POS/mix_embedding_24519.txt', index_col=0, header=None, skiprows=1, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "embed_type = 'mix'\n",
    "embed_dim = len(mix_model.columns)\n",
    "embed_voc = len(mix_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session 1 : 48 words among 725 not found in embedding mix\n",
      "Session 2 : 47 words among 812 not found in embedding mix\n",
      "Session 3 : 38 words among 860 not found in embedding mix\n",
      "Session 4 : 37 words among 762 not found in embedding mix\n",
      "Session 5 : 48 words among 732 not found in embedding mix\n",
      "Session 6 : 60 words among 902 not found in embedding mix\n",
      "Session 7 : 35 words among 819 not found in embedding mix\n",
      "Session 8 : 37 words among 712 not found in embedding mix\n",
      "Session 9 : 41 words among 802 not found in embedding mix\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,10):\n",
    "    session = pd.read_csv('./Context Lemma Onsets/' + str(i) + '_onset_wolf.csv')\n",
    "    session[\"duration\"] = session.offset - session.onset\n",
    "    features = pd.merge(session, mix_model, left_on='word', right_on=0, how='left')\n",
    "    print(\"Session\", i, \":\", sum(features.isna().any(axis=1)), \"words among\", len(features), \"not found in embedding\", embed_type)\n",
    "    features.dropna(inplace=True)\n",
    "    for d in range(1, embed_dim+1):\n",
    "        prefix = str(i) + '_' + embed_type + '_dim' + str(embed_dim) + '_voc' + str(embed_voc) + '_d'\n",
    "        features[['onset', 'duration', d]].rename(columns={d: 'amplitude'}).to_csv('./Features/' + prefix + str(d) + '.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct Frequency for newly combined words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexique_db = pd.read_csv('Lexique382.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,10):\n",
    "    session = pd.read_csv(str(i)+'_WOLF.csv', index_col=0)\n",
    "    session_fq_cw = pd.merge(session.loc[session.proposed_CW=='Y'], \n",
    "             lexique_db[['1_ortho', '3_lemme', '4_cgram', '9_freqfilms2']], \n",
    "             left_on=['word', 'proposed_lemma', 'proposed_categ'], \n",
    "             right_on=['1_ortho', '3_lemme', '4_cgram'], \n",
    "             how='left')[list(session.columns) + [\"9_freqfilms2\"]]\n",
    "    session_fq_cw.loc[session_fq_cw['9_freqfilms2'].notna(), 'logfreq'] = session_fq_cw.loc[session_fq_cw['9_freqfilms2'].notna(), '9_freqfilms2'].apply(np.log10)\n",
    "    session_fq_all = session.loc[session.proposed_CW=='N'].append(session_fq_cw[session_fq_cw.columns[:-1]], ignore_index=True, sort=False)\n",
    "    session_fq_all.sort_values(by=['onset', 'offset'], inplace=True)\n",
    "    session_fq_all.to_csv(str(i)+'_WOLF_fqdesamb.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Frequency Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,10):\n",
    "    session = pd.read_csv('./Word Onsets/' + str(i) + '_WOLF_fqdesamb.csv')\n",
    "    session[\"duration\"] = session.offset - session.onset\n",
    "    features = session[[\"onset\", \"duration\", \"logfreq\"]]\n",
    "    prefix = str(i) + '_freq'\n",
    "    features.rename(columns={'logfreq': 'amplitude'}).to_csv('./Features/' + prefix + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Word Rate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sying/root/anaconda3/envs/micipsa/lib/python3.6/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "CW_transformation = {\n",
    "    'Y': 1,\n",
    "    'N': 0\n",
    "}\n",
    "\n",
    "for i in range(1,10):\n",
    "    session = pd.read_csv('./Word Onsets/' + str(i) + '_WOLF_fqdesamb.csv')\n",
    "    session[\"duration\"] = session.offset - session.onset\n",
    "    features = session[[\"onset\", \"duration\", \"proposed_CW\"]]\n",
    "    features['amplitude'] = features['proposed_CW'].map(CW_transformation)\n",
    "    prefix = str(i) + '_cwrate'\n",
    "    features[[\"onset\", \"duration\", \"amplitude\"]].to_csv('./Features/' + prefix + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sying/root/anaconda3/envs/micipsa/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,10):\n",
    "    session = pd.read_csv('./Word Onsets/' + str(i) + '_WOLF_fqdesamb.csv')\n",
    "    session[\"duration\"] = session.offset - session.onset\n",
    "    features = session[[\"onset\", \"duration\"]]\n",
    "    features['amplitude'] = 1\n",
    "    prefix = str(i) + '_wrate'\n",
    "    features[[\"onset\", \"duration\", \"amplitude\"]].to_csv('./Features/' + prefix + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
